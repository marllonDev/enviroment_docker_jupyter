
# Enviroment to Data Engineer


This environment was created using docker. The docker image shows the Postgres database, PySpark, Java and Jupyter. It was developed to meet a need to run PySpark code quickly and without paid tools. Of course, everyone can do this, simply and easily with Docker.

## Installation

To run it, you must have the docker in your machine. After it, run the command:

```bash
docker-compose up --build
```
Once you have downloaded and configured the entire docker image, simply access the Jupyter Notebook via the following link -> http://localhost:8888/
    
## 🚀 About Me
𝐒𝐞𝐧𝐢𝐨𝐫 𝐃𝐚𝐭𝐚 𝐄𝐧𝐠𝐢𝐧𝐞𝐞𝐫

With 𝟰+ 𝘆𝗲𝗮𝗿𝘀 of expertise in the tech world, I thrive at the intersection of data engineering and innovation. Currently crafting scalable data ecosystems as a 𝗦𝗲𝗻𝗶𝗼𝗿 𝗗𝗮𝘁𝗮 𝗘𝗻𝗴𝗶𝗻𝗲𝗲𝗿, I’ve honed my skills across industries that shape economies—from 𝗕𝗿𝗮𝘇𝗶𝗹’𝘀 𝗹𝗮𝗿𝗴𝗲𝘀𝘁 𝗯𝗮𝗻𝗸𝘀 and 𝗴𝗹𝗼𝗯𝗮𝗹 𝗶𝗻𝘀𝘂𝗿𝗮𝗻𝗰𝗲 𝗹𝗲𝗮𝗱𝗲𝗿𝘀 to the 𝘄𝗼𝗿𝗹𝗱’𝘀 𝘁𝗼𝗽 𝗯𝗲𝗲𝗿 𝗽𝗿𝗼𝗱𝘂𝗰𝗲𝗿, now driving impact in the 𝗰𝗿𝗲𝗱𝗶𝘁 𝘀𝗲𝗰𝘁𝗼r. 

💡 𝗪𝗵𝘆 𝗜 𝗦𝘁𝗮𝗻𝗱 𝗢𝘂𝘁: \
I’ve 𝗮𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝗲𝗱 robust data pipelines 𝗳𝗼𝗿 𝗙𝗼𝗿𝘁𝘂𝗻𝗲 𝟱𝟬𝟬 𝗽𝗹𝗮𝘆𝗲𝗿𝘀, optimized legacy systems into cloud-native powerhouses (𝗔𝗪𝗦/𝗔𝘇𝘂𝗿𝗲), and delivered actionable insights through scalable ETL/ELT frameworks. From real-time financial analytics to brewery supply chain optimization, I turn raw data into strategic assets. 

✨ 𝗕𝗲𝘆𝗼𝗻𝗱 𝗖𝗼𝗱𝗲: \
A lifelong learner obsessed with data democratization and agile problem-solving. Let’s connect if you’re 𝗽𝗮𝘀𝘀𝗶𝗼𝗻𝗮𝘁𝗲 about cloud 𝗶𝗻𝗻𝗼𝘃𝗮𝘁𝗶𝗼𝗻, DevOps efficiency, or 𝗱𝗮𝘁𝗮’𝘀 role in transforming industries! 

Follow me: [Linkedin](https://www.linkedin.com/in/marllonzuc/) \
My Blog: [Blog](https://datatrends.me/)


![Logo](https://media.licdn.com/dms/image/v2/D4D03AQEFlFTNmApBhQ/profile-displayphoto-shrink_800_800/B4DZbt9iTrHsAc-/0/1747749054334?e=1753315200&v=beta&t=VfBvrDxLmoAYccE0DW63MbSLz_ao9Xp_HQAfcyP7-og)

